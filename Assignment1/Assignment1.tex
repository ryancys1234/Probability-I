\documentclass[12pt]{article}
\usepackage[left=.5in, right=.5in, top=.5in, bottom=.5in]{geometry}
\usepackage[parfill]{parskip}
\usepackage{amsmath, amssymb}
\pagenumbering{gobble}
\setlength\parindent{0pt}
\newcommand{\E}{\mathbb{E}}
\newcommand{\I}{\mathbb{I}}
\newcommand{\p}{\mathbb{P}}
\newcommand{\V}{\text{Var}}

\begin{document}

\begin{center}
{\Large STA347H1 - Assignment 1}
\end{center}

\textbf{1.11:} Let $A_1, A_2, \ldots \subseteq \omega$ and $w \in \Omega$. Define $B_i^w = A_i$ if $w \in A_i$ and $A_i^c$ if $w \notin A_i$. Notice that $\forall A_i$, $w$ either $\in A_i$ or $\notin A_i$. Thus, $\{B_i^w\}_{i=1}^\infty$ are independent. By the definition of $B_i^w$, $\{w\} \subseteq B_1^w, B_2^w, \ldots$, meaning that $\forall n \in \mathbb{N}^+$, $\{w\} \subseteq \cap_{i=1}^n B_i^w$. Then, $\p(\{w\}) \leq \p(\cap_{i=1}^n B_i^w) = \Pi_{i=1}^n \p(B_i^w) = (\frac{1}{2})^n \to 0$ as $n \to \infty$, which contradicts the assumption that $\Omega$ is countable.

\textbf{2.7:}
\begin{itemize}
    \item a) $F_X(x) = \p(YZ \leq x) = \p(YZ \leq x|Z = -1)\p(Z = -1) + \p(YZ \leq x|Z = 1)\p(Z = 1)$ by the law of total probability $= \frac{1}{2}(\p(-Y < x) + \p(Y < x)) = \frac{1}{2}(\p(Y > -x) + \p(Y < x)) = \p(Y < x) = F_Y(y)$ since $\p(Y > -x) = \p(Y < x)$ for the normal distribution. Differentiating both sides yields $f_X(x) = f_Y(x)$, so $X \sim \mathcal{N}(0, 1)$.

    Revised: Instead of differentiating, notice that Theorem 2.2.3 implies $X$ and $Y$ have the same distribution.
    \item b) $\p(|X| = |Y|) = \p(\{X = Y\} \cup \{X = -Y\}) = \p(X = Y) + \p(X = -Y)$ since the sets are disjoint $= \p(YZ = Y) + \p(YZ = -Y) = \p(Z = 1) + \p(Z = -1) = \frac{1}{2} + \frac{1}{2} = 1$.
    
    Revised: $\{w \in \Omega: |X(w)| = |Y(w)|\} = \{w \in \Omega: |Z(w)Y(w)| = |Y(w)|\} = \{w \in \Omega: |Z(w)||Y(w)| = |Y(w)|\} = \{w \in \Omega: |Y(w)| = |Y(w)|\} = \Omega \implies \p(|X| = |Y|) = 1$.
    \item c) Suppose $X$ and $Y$ are independent. Then, for any $A \in \mathcal{B}(\mathbb{R})$ such that $\{0\} \notin A$ and $\p(Y \in A) > 0$, $\p(\{X \in A\} \cap \{Y \in A\}) = \p(\{X \in A\} \cap \{Y \in A\}|Z = 1)\p(Z = 1) + \p(\{X \in A\} \cap \{Y \in A\}|Z = -1)\p(Z = -1) = \frac{1}{2} \p(\{Y \in A\} \cap \{Y \in A\}) + \frac{1}{2} \p(\{-Y \in A\} \cap \{Y \in A\}) = \frac{1}{2} \p(Y \in A) + \frac{1}{2}\p(\varnothing) = \frac{1}{2} \p(Y \in A) = \p(X \in A)\p(Y \in A)$ by assumption. Since $\p(Y \in A) > 0$, this implies $\p(X \in A) = \frac{1}{2}$, but this is a contradiction since $A$ is arbitrary.
\end{itemize}

\textbf{2.8:} Define $E_n = \{w \in \Omega: X(w) \geq \frac{1}{n}\} \nearrow E = \{w \in \Omega: X(w) > 0\}$, so that $\lim \limits_{n \to \infty} \p(E_n) = \p(\lim \limits_{n \to \infty} E_n) = \p(X > 0) > 0$ by the continuity of probability. Note that $\lim \limits_{n \to \infty} \p(E_n) > 0$ means that $\exists N \in \mathbb{N}$ such that $\forall n \geq N$, $\p(\{X \geq \frac{1}{n}\}) > 0$. The result then follows if $\delta = \frac{1}{N}$.

\textbf{2.9:} Let $A_1, A_2 \in \mathcal{B}(\mathbb{R})$ and define $B_1 = \{x \in \mathbb{R}: f(x) \in A_1\} \in \mathcal{B}(\mathbb{R})$ and $B_2 = \{y \in \mathbb{R}: g(y) \in A_2\} \in \mathcal{B}(\mathbb{R})$. Then, $\p(f(X) \in A_1 \cap g(Y) \in A_2) = \p(X \in \{x \in \mathbb{R}: f(x) \in A_1\} \cap Y \in \{y \in \mathbb{R}: g(y) \in A_2\}\} = \p(X \in B_1 \cap Y \in B_2) = \p(X \in B_1)\p(Y \in B_2)$ by the assumption of independence $= \p(f(X) \in A_1)\p(f(Y) \in A_2)$.

\textbf{2.11:} $X$ and $Y$ are random variables since they map from $\Omega$ to $\{0, 1\} \subseteq \mathbb{R}$. Also, notice that $\p(\{w \in \Omega: X(w) = 1\} \cap \{w \in \Omega: Y(w) = 1\}) = \p(\{w \in \Omega: \I(w \in A) = 1\} \cap \{w \in \Omega: \I(w \in B) = 1\}) = \p(A \cap B) = \p(A)\p(B)$ by assumption $= \p(\{w \in \Omega: \I(w \in A) = 1\})\p(\{w \in \Omega: \I(w \in B) = 1\}) = \p(\{w \in \Omega: X(w) = 1\})\p(\{w \in \Omega: Y(w) = 1\})$. By proposition 1.2.2, this equality also holds for the cases of $X = 0$ and $Y = 1$; $X = 1$ and $Y = 0$; and $X = Y = 0$. Thus, $X$ and $Y$ are independent.

\textbf{3.3:} Notice that $A_n \cap B_n \subseteq A_n \implies \cup_{k=n}^\infty (A_k \cap B_k) \subseteq \cup_{k=n}^\infty A_k \implies \cap_{n=1}^\infty \cup_{k=n}^\infty (A_k \cap B_k) \subseteq \cap_{n=1}^\infty \cup_{k=n}^\infty A_k \implies \limsup \limits_{n \to \infty} (A_n \cap B_n) \subseteq \limsup \limits_{n \to \infty} A_n$. Similarly, $\limsup \limits_{n \to \infty} (A_n \cap B_n) \subseteq \limsup \limits_{n \to \infty} B_n$. Thus, $\limsup \limits_{n \to \infty} (A_n \cap B_n) \subseteq (\limsup \limits_{n \to \infty} A_n) \cap (\limsup \limits_{n \to \infty} B_n)$. When $A_n = B_n$, $\limsup \limits_{n \to \infty} (A_n \cap B_n) = \limsup \limits_{n \to \infty} A_n = (\limsup \limits_{n \to \infty} A_n) \cap (\limsup \limits_{n \to \infty} B_n)$. When $\{A_i\}_i = \{B_j\}_j = \varnothing$ for odd $i$ and even $j$ and $\{B_j\}_j = \{A_k\}_k = \Omega$ for odd $j$ and even $k$, $\limsup \limits_{n \to \infty} (A_n \cap B_n) = \varnothing \subset \Omega = \Omega \cap \Omega = (\limsup \limits_{n \to \infty} A_n) \cap (\limsup \limits_{n \to \infty} B_n)$.

\textbf{3.6:} For $t \in [0, 1]$, construct the sequence $X_1 = \I(t \in [0, 1]), X_2 = \I(t \in [0, \frac{1}{2}]), X_3 = \I(t \in [\frac{1}{2}, 1]), X_4 = \I(t \in [0, \frac{1}{3}]), X_5 = \I(t \in [\frac{1}{3}, \frac{2}{3}])$, and so on. The chance that $X_n = 1$ decreases as $n \to \infty$, so $\{X_n\}_n$ converges in probability; however, $X_n = 1$ for infinity many $n$, so $\{X_n\}_n$ does not converge almost surely.

\textbf{3.7:} $X_n \to X$ a.s. $\iff \p(\lim \limits_{n \to \infty} X_n = X) = \p(\lim \limits_{n \to \infty} (X_n - X) = 0) = 1 \iff (X_n - X) \to 0$ a.s. $X_n \overset{p}{\to} X \iff \forall \varepsilon > 0, \lim \limits_{n \to \infty} \p(|X_n - X| \leq \varepsilon) = \lim \limits_{n \to \infty} \p(|(X_n - X) - 0| \leq \varepsilon) = 1 \iff (X_n - X) \overset{p}{\to} 0$.

\textbf{3.8:} Notice that $\forall \varepsilon > 0$, $\{w \in \Omega: |X_n(w) - a| \geq \varepsilon\} = \{w \in \Omega: |X_n(w) - a_n + a_n - a| \geq \varepsilon\} \subseteq \{w \in \Omega: |X_n(w) - a_n| + |a_n - a| \geq \varepsilon\}$ by the triangle inequality $\subseteq \{w \in \Omega: |X_n(w) - a_n| \geq \varepsilon\} \cup \{|a_n - a| \geq \varepsilon\}$. Thus, by lemma 1.1.11, $\lim \limits_{n \to \infty} \p(|X_n - a| \geq \varepsilon) \leq \lim \limits_{n \to \infty} \big[\p(|X_n - a_n| \geq \varepsilon) + \p(|a_n - a| \geq \varepsilon)\big] = \lim \limits_{n \to \infty} \p(|X_n - a_n| \geq \varepsilon) + \lim \limits_{n \to \infty} \p(|a_n - a| \geq \varepsilon) = 0$ by assumption.

\textbf{3.9:} By Theorem 3.2.7, there exists a subsequence $\{X_{n_k}\}_{n_k}$ such that $X_{n_k} \to X$ a.s. Notice that $\forall n \in \mathbb{N}^+, \exists k \in \mathbb{N}^+$ such that $n_{k} \leq n \leq n_{k+1}$. By monotonicity, $X_{n_{k}} \leq X_n \leq X_{n_{k+1}} \leq X \implies |X_n - X| \leq |X_{n_k} - X|$. Thus, $\forall \varepsilon > 0, \exists m \in \mathbb{N}$ such that $n \geq m \implies |X_{n_k} - X| \leq \varepsilon \implies |X_n - X| \leq \varepsilon$, so $X_n \to X$ a.s.

\textbf{4.3:}
\begin{itemize}
    \item Bernoulli: $\E(X) = 0p_X(0) + 1p_X(1) = 0(1 - p) + 1(p) = 1$. $\V(X) = \E(X^2) - \E(X)^2 = (0^2 p_X(0) + 1^2 p_X(1)) - p^2 = p - p^2 = p(1 - p)$.
    \item Binomial: Since $X \sim \Sigma_{i=1}^n X_i$ where each $X_i \sim$ Bernoulli($p$) and is independent, $\E(X) = \Sigma_{i=1}^n \E(X_i)$ since the sum is finite $= np$ and $\V(X) = \Sigma_{i=1}^n \V(X_i)$ by independence $= np(1 - p)$.
    \item Poisson: $\E(X) = \Sigma_{x=0}^\infty x\frac{\lambda^x e^{-\lambda}}{x!} = \lambda e^{-\lambda} \Sigma_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!} = \lambda e^{-\lambda} \Sigma_{x=0}^\infty \frac{\lambda^x}{x!} = \lambda e^{-\lambda} e^{\lambda}$ by the exponential power expansion $= \lambda$. $\E(X(X-1)) = \Sigma_{x=0}^\infty x(x-1)\frac{\lambda^x e^{-\lambda}}{x!} = \lambda^2 e^{-\lambda} \Sigma_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!} = \lambda^2 e^{-\lambda} \Sigma_{x=0}^\infty \frac{\lambda^x}{x!} = \lambda^2 e^{-\lambda} e^{\lambda}$ by the exponential power expansion $= \lambda^2$. $\V(X) = \E(X^2) - \E(X)^2 = \E(X^2 - X) + \E(X) - \E(X)^2 = \lambda^2 + \lambda - \lambda^2 = \lambda$.
\end{itemize}

\textbf{4.4:}
\begin{itemize}
    \item Bernoulli: $\E(e^{\lambda X}) = e^0 p_X(0) + e^\lambda p_X(1) = 1 - p + e^\lambda p$.
    \item Binomial: Since $X \sim \Sigma_{i=1}^n X_i$ where each $X_i \sim$ Bernoulli($p$) and is independent, by Lemma 4.4.4, $\E(e^{\lambda X}) = \Pi_{i=1}^n \E(e^{\lambda X_i}) = (1 - p + e^\lambda p)^n$.
    \item Poisson: $\E(e^{tX}) = \Sigma_{x=0}^\infty e^{tx} \frac{\lambda^x e^{-\lambda}}{x!} = e^{-\lambda} \Sigma_{x=0}^\infty \frac{(\lambda e^t)^x}{x!} = e^{-\lambda} \exp(\lambda e^t)$ by the exponential power expansion $= \exp(\lambda (e^t - 1))$.
\end{itemize}

\textbf{4.7:} The MGF of the normal distribution is $M_{X}(\lambda) = \exp(\mu\lambda + \frac{1}{2}\sigma^2\lambda^2)$ for some $X \sim \mathcal{N}(\mu, \sigma^2)$. By Lemma 4.4.4, for $S = \Sigma_{i=1}^n X_i$, $M_S(\lambda) = \Pi_{i=1}^n M_{X_i}(\lambda) = \Pi_{i=1}^n \exp(\mu\lambda + \frac{1}{2}\sigma^2\lambda^2) = \exp(\Sigma_{i=1}^n (\mu\lambda + \frac{1}{2}\sigma^2\lambda^2)) = \exp(n\mu\lambda + \frac{n}{2}\sigma^2\lambda^2))$. Thus, $S \sim \mathcal{N}(n\mu, n\sigma^2)$.

Revised: $\exp(n\mu\lambda + \frac{n}{2}\sigma^2\lambda^2))$ is the MGF of some $Y \sim \mathcal{N}(n\mu, n\sigma^2)$. Additionally, $\forall \lambda, M_S(\lambda) < \infty$. By Theorem 4.4.3, $S$ and $Y$ have the same distribution.

\textbf{4.10:} Notice that $X \geq x\I(X \geq x)$ $\forall x > 0$, so $0 = \E(X) \geq \E(x\I(X \geq x)) = x\E(\I(X \geq x)) = x\p(X \geq x)$ by the properties of expectation, implying $\p(X \geq x) = 0$. Then, $\p(X > 0) = \p(\cup_{n=1}^\infty \{X \geq \frac{1}{n}\}) \leq \Sigma_{n=1}^\infty \p(X \geq \frac{1}{n}) = 0$, so $\p(X = 0) = \p(X \geq 0) - \p(X > 0) = 1$. Alternatively, suppose that $\p(X > 0) > 0 \iff \exists \varepsilon > 0, \delta > 0$ s.t. $\p(X \geq \delta) = \varepsilon$. Notice that $\E(X) = \E(X\mathbb{I}(X < \delta)) + \E(X\mathbb{I}(X \geq \delta)) \geq \E(X\mathbb{I}(X \geq \delta)) \geq \E(\delta \mathbb{I}(X \geq \delta)) = \delta \p(X \geq \delta) = \delta \varepsilon > 0$, a contradiction.

\textbf{4.14:} Define $Y = \Sigma_{i=1}^N X_i$. By the law of total expectation, $\E(Y) = \E(\E(Y|N)) = \Sigma_{n=1}^\infty \E(Y|N = n)\p(N = n) = \Sigma_{n=1}^\infty (\Sigma_{i=1}^n \E(X_i|N = n))\p(N = n)$ since the inner sum is finite $= \Sigma_{n=1}^\infty (\Sigma_{i=1}^n \E(X_i))\p(N = n)$ by independence $= \Sigma_{n=1}^\infty n\mu\p(N=n) = \mu \Sigma_{n=1}^\infty n\p(N=n) = \mu\E(N) = \mu m$. Notice that $\E_N(\V(Y|N)) = \E_N(\Sigma_{i=1}^N \V(X_i))$ by independence $= \E_N(N\V(X_i)) = \V(X_i)\E_N(N) = \sigma^2 m$ and $\V_N(\E(Y|N)) = \V_N(\Sigma_{i=1}^N \E(X_i|N))$ since the inner sum is finite $= \V_N(\Sigma_{i=1}^N \E(X_i))$ by independence $= \V_N(N\E(X_i)) = \E(X_i)^2\V(N) = \mu^2 v$. The result follows by the law of total variance.

\textbf{4.18:} Define $Y_t = t\I(X > t)$ $\forall t > 0$. Notice that $\I(X > t) = 0$ for large enough $t \implies \p(\lim \limits_{t \to \infty} t\I(X > t) = 0 < X) = 1$. Thus, $Y_n \to 0$ a.s. and $|Y_n| \leq X$ a.s., so by the DCT, $0 = \lim \limits_{t \to \infty} \E(Y_t) = \lim \limits_{t \to \infty} t\p(X > t)$.

\end{document}
